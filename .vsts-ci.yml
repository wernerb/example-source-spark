steps:
- task: DockerCompose@0
  displayName: tests
  inputs:
    dockerComposeCommand: |
      run --rm sbt sbt test
- task: DockerCompose@0
  displayName: package fat jar
  inputs:
    dockerComposeCommand: |
      run --rm sbt sbt assembly
  condition: and(succeeded(), ne(variables['Build.Reason'], 'PullRequest'))
- task: CopyFiles@2
  displayName: |
    copy files for publishing
  inputs:
    #target/scala-2.11/example-source-spark-assembly-0.1.jar
    Contents: 'target/**/*.jar'
    TargetFolder: '$(build.artifactstagingdirectory)'
  condition: and(succeeded(), ne(variables['Build.Reason'], 'PullRequest'))
- task: PublishBuildArtifacts@1
  displayName: |
    publish files to vsts
  inputs:
    pathtoPublish: '$(Build.ArtifactStagingDirectory)' 
    artifactName: 'example-source-spark' 
    publishLocation: 'container' 
  condition: and(succeeded(), ne(variables['Build.Reason'], 'PullRequest'))
- task: DockerCompose@0
  displayName: deploy to databricks
  env:
    DATABRICKS_TOKEN: $(databricks_token)
    DATABRICKS_HOST: $(databricks_host)
  inputs:
    dockerComposeCommand: |
      run --rm db /mount/deploy.sh
  condition: and(succeeded(), ne(variables['Build.Reason'], 'PullRequest'))